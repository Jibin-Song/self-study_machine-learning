# -*- coding: utf-8 -*-
"""로지스틱 회귀.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q9xlAM0X0--lvworaovO77xfTIoXUbXc

### 작성 날짜: 2021.09.09(목)
### 프로그램 개요: 로지스틱 회귀 분류 - 생선의 특성을 이용하여 생선의 종류를 확률로 예측하기

# 데이터 준비
"""

#판다스로 데이터 프레임 만들기
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')    
fish.head()   #head() 메서드로 처음 5개 행 출력

#unique() --> 각 행의 종류를 알려줌
print(pd.unique(fish['Species']))
print(pd.unique(fish['Weight']))

#입력 데이터: Weight, Length, Diagonal, Height, Width
#타깃 데이터: Speciesr
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
print(fish_input[:5])
print(fish_target[:5])

#Train Set, Test Set 나누기
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

"""# 최근접 이웃 분류기"""

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

#최근접 이웃 분류기의 확률 예측
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(train_scaled, train_target)
print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target)) #과소적합 --> n을 낮춰 복잡도 올리기

kn = KNeighborsClassifier(n_neighbors=3) 
kn.fit(train_scaled, train_target)
print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))
print(kn.classes_)    #정렬된 타깃값

print(kn.predict(test_scaled[:5]))

import numpy as np
proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))

"""### 최적의 n_neighbors 찾기"""

import numpy as np

train_score = []
test_score = []
n_list = np.arange(1,11)

for n in n_list:
  kn = KNeighborsClassifier(n_neighbors=n)
  kn.fit(train_scaled, train_target)
  train_score.append(kn.score(train_scaled, train_target))
  test_score.append(kn.score(test_scaled, test_target))

max_score = test_score[0]
max_n = n_list[0]
for i in range(len(test_score)):
  if(max_score <= test_score[i] and test_score[i] <= train_score[i]):
    max_score = test_score[i]
    max_n = n_list[i]

import matplotlib.pyplot as plt
plt.plot(n_list, train_score)
plt.plot(n_list, test_score)
plt.scatter(max_n, max_score)
plt.xlabel('n_neighbor')
plt.ylabel('R^2')
plt.show()

"""# 로지스틱 회귀 분류

###시그모이드 함수 (로지스틱 함수) 그리기
"""

import numpy as np
import matplotlib.pyplot as plt
z = np.arange(-5,5,0.1)
phi = 1 / (1 + np.exp(-z))
print(phi)
plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()

"""###로지스틱 회귀 - 이진 분류"""

#불리언 인덱싱: 넘파이 배열은 True, False 값을 전달하여 행을 선택할 수 있다.
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])

#도미와 빙어만 뽑아 이진 분류할 데이터 준비
print(train_target == 'Bream')    # train_target 리스트에서 Bream에 해당하는 부분을 True, 그렇지않으면 False 리스트 만듦
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
print('\n', bream_smelt_indexes)
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
print(train_bream_smelt)
print(target_bream_smelt)

#이진 분류 로지스틱 회귀 훈련, 예측, 평가

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
print(lr.predict(train_bream_smelt[:5]))
print(lr.predict_proba(train_bream_smelt[:5]))
print(lr.score(train_bream_smelt, target_bream_smelt))

print(lr.classes_)

#로지스틱 회귀가 학습한 계수 확인
print(lr.coef_, lr.intercept_)

#각 샘플의 z값 구하기(시그모이드 함수의 input 값)
decision = lr.decision_function(train_bream_smelt[:5])
print(np.round(decision, decimals=2))

#시그모이드 함수와 각 샘플의 함수 값 그리기
import matplotlib.pyplot as plt
import numpy as np
z = np.arange(-7,7,0.1)
def phi(z):
  return 1 / (1 + np.exp(-z))
plt.plot(z, phi(z))
plt.axvline(x = 0, color = 'black', linewidth = 1, linestyle = ':')
plt.xlabel('z')
plt.ylabel('phi')
plt.scatter(decision, phi(decision), color = 'g')
plt.show()

#시그모이드 함수를 거친 확률 값
from scipy.special import expit
print(expit(decision)) #proba의 양성 클래스의 확률값과 같음

"""### 로지스틱 회귀 - 다중 분류"""

lr = LogisticRegression(C=20, max_iter=1000) #C 클수록 규제 약해짐 // max_iter은 반복 횟수 지정
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

print(lr.predict(test_scaled[:5]))

proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))

#정렬된 타깃 값
print(lr.classes_)
#로지스틱 회귀 계수 값 확인
print(lr.coef_, lr.intercept_)

#(다중회귀에선)소프트맥스 함수의 z값
decision = lr.decision_function(test_scaled[:5])
print(decision)

print(np.round(decision, decimals=2))

from scipy.special import softmax
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=2))